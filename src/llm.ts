import { Ollama } from "@langchain/community/llms/ollama";

import path, { dirname } from "path";
import fs from "fs-extra";
import { fileURLToPath } from "url";
import { BedrockModelAwsKey, OpenAIModel } from "./app/type.js";
import { PromptTemplate } from "@langchain/core/prompts";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const configPath = path.join(__dirname, 'config.json');
import { BedrockChat } from "@langchain/community/chat_models/bedrock";
import { ChatOpenAI } from "@langchain/openai";

const JSON_TEMPLATE = `
- For the json file format, the translated text do not change the keys and only translate the values. Do not remove $$$$$$$ from the keys.
- For the json file format, the translated text every key is very important, do not miss any key.
- For  the json file format, the translated text should add closing curly brackets to nested object if it is missed. This has been missed multiple times.
- For the json file format, the translated text do not change the case of the keys. Keep the keys in the same case as they are in the text input.
- For the json file format, the translated content is directly sent to JSON.parse() function. Make sure to return only valid JSON data or it will failed to parse.
- For the json file format, don't use "，", use comma "," instead.
`

const MDX_TEMPLATE = `
- For the mdx file format, the header chunk is in between "---" lines. Do not change keys of header chunk and only translate the values.
- For the mdx file format, do not add header chunk if it is not present in the input text chunk.
- For the mdx file format, the translated text should not worry about the perfect translation.
`

const getTranslateTemplate = (fileFormat: string) => {
	let additionalInstruction = "";
	if (fileFormat === "json") {
		additionalInstruction = JSON_TEMPLATE;
	} else if (fileFormat === "mdx") {
		additionalInstruction = MDX_TEMPLATE;
	}
	return `
Instructions for Translation:
- Translate the following sentence from English to {destLang}
- Keep the file format {fileFormat} same.
- The translated text will be directly written to the file.
- The translated text should not worry about the perfect translation.
- The input text is coming as a chunk of text and it will be coming in the same format in the output. Don't add anything apart from the translation to avoid wrong message interpretation.
${additionalInstruction}
- The translated text should not worry about the perfect translation.
- The translated text should not break the unique words into parts and translate e.g. NextGenAIKit -> Next Gen AI Kit is wrong, it should be NextGenAIKit.
- The translated content will be send to end user without human intervention. You should not include any line which end user can think that this content is generated by the AI e.g. Here is the translation of the given text from English to or similar kind of messages.
- The translated text should not ask for any feedback and should not include any remarks.
- The translated text should not add any Note or Warning: messages to the translation.
- The translated text should not add any introductory or explanatory or warning messages in the response. This will fail further processing.
- The translated text should return only translated message without any additional messages generated by you. It could be Note: or Warning: or general message.
- You should not add any disclaimers or specific types of messages appended to the response.

Original Text Content:
{text}
`;
}


export const translate = async (text: string, destLang: string, fileFormat: string = "text") => {
	const configuration = fs.readJsonSync(configPath) as any;
	const model = getLLMModel();
	const prompt = PromptTemplate.fromTemplate(formatPrompt(getTranslateTemplate(fileFormat)));
	// console.log("Prompt:", prompt)

	// @ts-ignore
	const chain = prompt.pipe(model);
	const response = await chain.invoke({
		destLang,
		text,
		fileFormat,
	}) as any;

	switch (configuration.llm) {
		case "ollama":
			let ollamaContent = response;
			if (fileFormat === "json") {
				// remove new line and extra spaces
				const commentRegex = /^\/\/.*\n?/gm;
				const newLineRegex = /[\n\r]/g;
				ollamaContent =  response.replace(commentRegex, "").replace(newLineRegex, "").replace("，", ",");
				console.log(ollamaContent);
			}
			const regex1 = /Here is the translation \w.+/gi; // remove the full like
			const regex2 = /Note: \w.+/gi; // remove the full like
			const regex3 = /Sure, \w.+/gi; // remove the full like
			const regex4 = /^.*Translation: \w.+/;
			return ollamaContent.replace(regex1, "").replace(regex2, "").replace(regex3, "").replace(regex4, "");
		case "bedrock":
			const content = response?.content;
			if (fileFormat === "json") {
				// remove new line and extra spaces
				const newLineRegex = /[\n\r]/g;
				return content.replace(newLineRegex, "");
			}
			return content;
		case "open-ai":
			const openAIContent = response?.content;
			if (fileFormat === "json") {
				// remove new line and extra spaces
				const newLineRegex = /[\n\r]/g;
				return openAIContent.replace(newLineRegex, "");
			}
			return openAIContent;
		default:
			return response;
	}
}


const getLLMModel = () => {
	const configuration = fs.readJsonSync(configPath) as any;
	if (configuration.llm) {
		if (configuration.llm === "ollama") {
			const newConfig = configuration.config as Ollama;
			return new Ollama({
				temperature: 0.3,
				model: newConfig.model,
			});
		}
		if (configuration.llm === "bedrock") {
			const newConfig = configuration.config as BedrockModelAwsKey;
			return new BedrockChat({
				// model: "anthropic.claude-3-sonnet-20240229-v1:0",
				model: newConfig.model, // "anthropic.claude-3-haiku-20240307-v1:0", // "anthropic.claude-3-sonnet-20240229-v1:0
				region: newConfig.region,
				// endpointUrl: "custom.amazonaws.com",
				credentials: {
					accessKeyId: newConfig.accessKey,
					secretAccessKey: newConfig.secretKey,
				},
				// modelKwargs: {
				//   anthropic_version: "bedrock-2023-05-31",
				// },
			});
		}
		if (configuration.llm === "open-ai") {
			const newConfig = configuration.config as OpenAIModel;
			return new ChatOpenAI({
				openAIApiKey: newConfig.apiKey,
				modelName: newConfig.model,
			});
		}

	}
	throw new Error("LLM not found in configuration");
};


export const formatPrompt = (prompt: string) => {
	const configuration = fs.readJsonSync(configPath) as any;
	if (configuration.llm) {
		if (configuration.llm === "bedrock") {
			return `Human: ${prompt}\n Assistant: \n`;
		}
	}
	return prompt;
}

export default getLLMModel;
